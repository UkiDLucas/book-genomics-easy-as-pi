# Hadoop

## Distributed Operating System

When considering a cluster computer, the main architectural solution would be to use a distributed software model. Distributed programming takes a Big Data processing task and breaks it into smaller size processes that can run on separate CPU cores. The advantage of correctly designed distributed approach is that if any process fails it can be re-run on different processor without affecting the final outcome. In well designed distributed computer adding more computers, or nodes, will scale up the architecture in such a way that subsequent processes can be assigned available CPU cores faster. Another benefit of distributed computing is that each process takes a small chunk of data, e.g. 64-124MB instead of 3GB as it is often the case with whole DNA sequences. Such small processes can be easily run on inexpensive hardware such as Pi single board  Linux computers. These boards can be rapidly scaled up to processing needs by adding a board at the cost of $16-$30 per 4 CPU unit. 